{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Крестики-нолики\n",
    "### Для начала построим саму модель, то есть определим все состояния и переходы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def log_progress(sequence, every=None, size=None):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{index} / ?'.format(index=index)\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{index} / {size}'.format(\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = str(index or '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_equal(a, b):\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    a = np.rot90(a)\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    a = np.rot90(a)\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    a = np.rot90(a)\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    a = np.rot90(a)\n",
    "    a = np.fliplr(a)\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    a = np.rot90(a)\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    a = np.rot90(a)\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    a = np.rot90(a)\n",
    "    if np.array_equal(a, b):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def game_result(state):\n",
    "    for res in [-1, 1]:\n",
    "        if np.any(np.all(state == res, axis=1)):\n",
    "            return res\n",
    "        if np.any(np.all(state == res, axis=0)):\n",
    "            return res\n",
    "        if state[0, 0] == res and state[1, 1] == res and state[2, 2] == res:\n",
    "            return res\n",
    "        if state[0, 2] == res and state[1, 1] == res and state[2, 0] == res:\n",
    "            return res\n",
    "    if np.all(state != 0):\n",
    "        return 0\n",
    "    return 2 # not finished\n",
    "\n",
    "def generate_states():\n",
    "    global generate_states_return_value\n",
    "    if 'generate_states_return_value' in globals():\n",
    "        return generate_states_return_value\n",
    "    states = []\n",
    "    actions = {}\n",
    "    groups = []\n",
    "    prev = 0\n",
    "    cur = 1\n",
    "    end = 1\n",
    "    states.append(np.zeros((3, 3), dtype='int8'))\n",
    "    for number in range(0, 9):\n",
    "        groups.append(end)\n",
    "        for state in range(prev, cur):\n",
    "            if game_result(states[state]) != 2:\n",
    "                continue\n",
    "            for i in range(states[state].shape[0]):\n",
    "                for j in range(states[state].shape[1]):\n",
    "                    if states[state][i, j] == 0:\n",
    "                        a = states[state].copy()\n",
    "                        a[i, j] = 1 - 2*(number % 2)\n",
    "                        unique = True\n",
    "                        for old_state in range(cur, end):\n",
    "                            if is_equal(states[old_state], a):\n",
    "                                new_state = old_state\n",
    "                                unique = False\n",
    "                                break\n",
    "                        if unique == True:\n",
    "                            new_state = end\n",
    "                            states.append(a)\n",
    "                            end += 1\n",
    "                        act = actions.get(state, [])\n",
    "                        if new_state not in act:\n",
    "                            act.append(new_state)\n",
    "                        actions[state] = act\n",
    "        prev = cur\n",
    "        cur = end\n",
    "    groups = [0] + groups + [len(states)]\n",
    "    generate_states_return_value = (states, actions, groups)\n",
    "    return generate_states_return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь реализуем алгоритмы SARSA и Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    def __init__(self, states, actions, side, alpha, trainable=True):\n",
    "        self.trainable = trainable\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.eps = 0\n",
    "        if side == 'x':\n",
    "            self.side = 1\n",
    "        else:\n",
    "            self.side = -1\n",
    "        self.alpha = alpha\n",
    "        self.action_values = {}\n",
    "        for key in actions.keys():\n",
    "            self.action_values[key] = np.zeros(len(actions[key]))\n",
    "        self.step = 1\n",
    "\n",
    "    def proba(self, state):\n",
    "#         if self.eps >= 0.0001:\n",
    "#             self.eps = 1 / self.step ** 0.5\n",
    "        proba = np.zeros_like(self.action_values[state])\n",
    "        best_args = self.action_values[state] == self.action_values[state].max()\n",
    "        proba[best_args] = (1 - self.eps) / np.argwhere(best_args).shape[0]\n",
    "        proba += self.eps / proba.shape[0]\n",
    "        return proba\n",
    "\n",
    "\n",
    "    def choose(self, state):\n",
    "        self.old_state = state\n",
    "        self.last_action = np.random.choice(len(self.actions[state]), p=self.proba(state))\n",
    "        return self.actions[state][self.last_action]\n",
    "\n",
    "    def reward(self, new_state, reward):\n",
    "        if self.trainable == False:\n",
    "            return 0.0\n",
    "        reward *= self.side\n",
    "        old_value = self.action_values[self.old_state][self.last_action]\n",
    "        next_value = 0\n",
    "        if len(self.actions.get(new_state, [])) > 0:\n",
    "            new_choice = np.random.choice(len(self.actions[new_state]), p=self.proba(new_state))\n",
    "            next_value = self.action_values[new_state][new_choice]\n",
    "        diff = self.alpha * (reward + next_value - old_value)\n",
    "        self.action_values[self.old_state][self.last_action] += diff\n",
    "        self.step += 1\n",
    "        return abs(diff)\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, states, actions, side, alpha, trainable=True):\n",
    "        self.trainable = trainable\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.eps = 0\n",
    "        if side == 'x':\n",
    "            self.side = 1\n",
    "        else:\n",
    "            self.side = -1\n",
    "        self.alpha = alpha\n",
    "        self.action_values = {}\n",
    "        for key in actions.keys():\n",
    "            self.action_values[key] = np.zeros(len(actions[key]))\n",
    "        self.step = 1\n",
    "\n",
    "    def proba(self, state):\n",
    "#         if self.eps >= 0.0001:\n",
    "#             self.eps = 1 / self.step ** 0.5\n",
    "        proba = np.zeros_like(self.action_values[state])\n",
    "        best_args = self.action_values[state] == self.action_values[state].max()\n",
    "        proba[best_args] = (1 - self.eps) / np.argwhere(best_args).shape[0]\n",
    "        proba += self.eps / proba.shape[0]\n",
    "        return proba\n",
    "\n",
    "\n",
    "    def choose(self, state):\n",
    "        self.old_state = state\n",
    "        self.last_action =  np.random.choice(len(self.actions[state]), p=self.proba(state))\n",
    "        return self.actions[state][self.last_action]\n",
    "\n",
    "    def reward(self, new_state, reward):\n",
    "        if self.trainable == False:\n",
    "            return 0.0\n",
    "        reward *= self.side\n",
    "        old_value = self.action_values[self.old_state][self.last_action]\n",
    "        next_value = 0\n",
    "        if len(self.actions.get(new_state, [])) > 0:\n",
    "            next_value = self.action_values[new_state].max()\n",
    "        diff = self.alpha * (reward + next_value - old_value)\n",
    "        self.action_values[self.old_state][self.last_action] += diff\n",
    "        self.step += 1\n",
    "        return abs(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определим заодно класс, который позволит играть с ними человеку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def __init__(self, side):\n",
    "        self.states, self.actions = generate_states()[:2]\n",
    "        if side == 'x':\n",
    "            self.marker = 1\n",
    "        else:\n",
    "            self.marker = -1\n",
    "\n",
    "    def choose(self, state):\n",
    "        row, col = map(int, input('row column (from 1 to 3): ').split())\n",
    "        row -= 1\n",
    "        col -= 1\n",
    "        a = self.states[state].copy()\n",
    "        a[row, col] = self.marker\n",
    "        new_state = state + 1\n",
    "        while True:\n",
    "            if is_equal(a, self.states[new_state]):\n",
    "                break\n",
    "            new_state += 1\n",
    "        return new_state\n",
    "\n",
    "    def reward(self, new_state, reward):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, определим класс, которай будет управлять самой игрой и обучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, player_x_class, player_o_class, alpha):\n",
    "        self.states, self.actions, self.state_groups = generate_states()\n",
    "        x_states = []\n",
    "        o_states = []\n",
    "        x_actions = {}\n",
    "        o_actions = {}\n",
    "        for i in range(0, len(self.state_groups) - 1, 2):\n",
    "            x_states.extend(list(range(self.state_groups[i], self.state_groups[i+1])))\n",
    "            o_states.extend(list(range(self.state_groups[i+1], self.state_groups[i+2])))\n",
    "        for state in x_states:\n",
    "            x_actions[state] = self.actions.get(state, [])\n",
    "        for state in o_states:\n",
    "            o_actions[state] = self.actions.get(state, [])\n",
    "\n",
    "        self.player_x = player_x_class(x_states, x_actions, 'x', alpha=alpha)\n",
    "        self.player_o = player_o_class(o_states, o_actions, 'o', alpha=alpha)\n",
    "\n",
    "    def train(self, games_num, epsilon=0.2, with_epsilon=1, verbose=False, counter=0, graph=False):\n",
    "        players = [self.player_x, self.player_o]\n",
    "        x_wins = 0\n",
    "        o_wins = 0\n",
    "        draws = 0\n",
    "        diffs = [[], []]\n",
    "        players[0].eps = epsilon\n",
    "        players[1].eps = epsilon\n",
    "        if counter > 0:\n",
    "            games_range = log_progress(range(games_num), every=counter)\n",
    "        else:\n",
    "            games_range = range(games_num)\n",
    "        for game in games_range:\n",
    "            if game > games_num * with_epsilon:\n",
    "                players[0].eps = 0\n",
    "                players[1].eps = 0\n",
    "            turn = 0\n",
    "            state = 0\n",
    "            if verbose:\n",
    "                # print(self.states[state])\n",
    "                print(state)\n",
    "            while game_result(self.states[state]) == 2:\n",
    "                state = players[turn % 2].choose(state)\n",
    "                if verbose:\n",
    "                    # print(self.states[state])\n",
    "                    print(state)\n",
    "                if turn >= 1:\n",
    "                    reward = game_result(self.states[state])\n",
    "                    if reward == 2:\n",
    "                        reward = 0\n",
    "                    diff = players[(turn+1) % 2].reward(state, reward)\n",
    "                    diffs[(turn + 1) % 2].append(diff)\n",
    "                turn += 1\n",
    "            diff = players[(turn+1) % 2].reward(state, reward)\n",
    "            diffs[(turn+1) % 2].append(diff)\n",
    "            reward = game_result(self.states[state])\n",
    "            if reward == 0:\n",
    "                draws += 1\n",
    "            elif reward == 1:\n",
    "                x_wins += 1\n",
    "            else:\n",
    "                o_wins += 1\n",
    "\n",
    "            if verbose:\n",
    "                print('=======================================')\n",
    "        if graph:\n",
    "            plt.subplot(211)\n",
    "            plt.plot(range(len(diffs[0])), diffs[0], 'r')\n",
    "            plt.subplot(212)\n",
    "            plt.plot(range(len(diffs[1])), diffs[1], 'b')\n",
    "            plt.show()\n",
    "        return (x_wins, o_wins, draws, games_num)\n",
    "\n",
    "    def play(self, games_num, computer):\n",
    "        if computer == 'x':\n",
    "            human = Human('o')\n",
    "            players = [self.player_x, human]\n",
    "        else:\n",
    "            human = Human('x')\n",
    "            players = [human, self.player_o]\n",
    "        for game in range(games_num):\n",
    "            turn = 0\n",
    "            state = 0\n",
    "            while game_result(self.states[state]) == 2:\n",
    "                print(self.states[state])\n",
    "                state = players[turn % 2].choose(state)\n",
    "                turn += 1\n",
    "            print(self.states[state])\n",
    "            if game_result(self.states[state]) == 1:\n",
    "                print(\"Winner: X\")\n",
    "            elif game_result(self.states[state]) == -1:\n",
    "                print(\"Winner: O\")\n",
    "            else:\n",
    "                print(\"Draw\")\n",
    "            print('=======================================')\n",
    "            print('=======================================')\n",
    "\n",
    "    def change_player(self, player, new_player):\n",
    "        if player == 'x':\n",
    "            self.player_x = new_player\n",
    "        else:\n",
    "            self.player_o = new_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь посмотрим на алгоритмы в деле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% are draws\n"
     ]
    }
   ],
   "source": [
    "sarsa_game = TicTacToe(SARSA, SARSA, 0.2)\n",
    "sarsa_game.train(20000, epsilon=0.2, with_epsilon=0.9, counter=100)\n",
    "result = sarsa_game.train(1000, epsilon=0)\n",
    "print(result[2]/result[3] * 100, '% are draws', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Попробуем выиграть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "row column (from 1 to 3): 2 2\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "row column (from 1 to 3): 1 3\n",
      "[[ 1  0 -1]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "[[ 1  0 -1]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -1]]\n",
      "row column (from 1 to 3): 2 3\n",
      "[[ 1  0 -1]\n",
      " [ 0  1  1]\n",
      " [ 0  0 -1]]\n",
      "[[ 1 -1  0]\n",
      " [ 0  1  0]\n",
      " [-1  1 -1]]\n",
      "row column (from 1 to 3): 2 3\n",
      "[[ 1 -1  0]\n",
      " [ 0  1  1]\n",
      " [-1  1 -1]]\n",
      "[[ 1 -1 -1]\n",
      " [-1  1  1]\n",
      " [ 0  1 -1]]\n",
      "row column (from 1 to 3): 3 1\n",
      "[[ 1 -1  1]\n",
      " [-1  1  1]\n",
      " [-1  1 -1]]\n",
      "Draw\n",
      "=======================================\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# Интерактивный режим\n",
    "sarsa_game.play(1, computer='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь будем намеренно поддаваться (очевидно и не очень):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "row column (from 1 to 3): 1 2\n",
      "[[ 1 -1  0]\n",
      " [ 0  0  0]\n",
      " [ 0  0  0]]\n",
      "[[ 1 -1  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "row column (from 1 to 3): 2 3\n",
      "[[ 1 -1  0]\n",
      " [ 0  1 -1]\n",
      " [ 0  0  0]]\n",
      "[[ 1 -1  0]\n",
      " [ 0  1 -1]\n",
      " [ 0  0  1]]\n",
      "Winner: X\n",
      "=======================================\n",
      "=======================================\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "row column (from 1 to 3): 2 2\n",
      "[[ 1  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "[[ 1  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  1]]\n",
      "row column (from 1 to 3): 1 3\n",
      "[[ 1  0 -1]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  1]]\n",
      "[[ 1  0 -1]\n",
      " [ 0 -1  0]\n",
      " [ 1  0  1]]\n",
      "row column (from 1 to 3): 1 2\n",
      "[[ 1 -1 -1]\n",
      " [ 0 -1  0]\n",
      " [ 1  0  1]]\n",
      "[[ 1 -1 -1]\n",
      " [ 0 -1  0]\n",
      " [ 1  1  1]]\n",
      "Winner: X\n",
      "=======================================\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "sarsa_game.play(2, computer='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, **SARSA** играет против живого игрока очень уверенно. Даже более того: алгоритм никогда не проигрывает, поэтому с самим собой он всегда играет в ничью\n",
    "Посмотрим теперь, как ему противостоит Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% are draws\n"
     ]
    }
   ],
   "source": [
    "q_game = TicTacToe(QLearning, QLearning, 0.2)\n",
    "q_game.train(20000, epsilon=0.2, with_epsilon=0.9, counter=100)\n",
    "result = q_game.train(1000, epsilon=0)\n",
    "print(result[2]/result[3] * 100, '% are draws', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game = TicTacToe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  },
  "widgets": {
   "state": {
    "3293b182b2ff4512981fcf5c3844c75b": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    },
    "d1937e51e8a64e0cade625a934c0d012": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
