{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas\n",
    "from sympy import init_printing\n",
    "from IPython.display import display\n",
    "from numpy import genfromtxt\n",
    "from mpmath import diff\n",
    "import time\n",
    "import functools\n",
    "init_printing(use_latex='mathjax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('train.csv', sep=',').as_matrix()\n",
    "samples = data[:35000, 1:]\n",
    "prepared_samples = np.hstack((samples, np.ones((samples.shape[0], 1))))\n",
    "labels = data[:35000, 0]\n",
    "\n",
    "test_samples = data[35000:, 1:]\n",
    "prepared_test_samples = np.hstack((test_samples, np.ones((test_samples.shape[0], 1))))\n",
    "test_labels = data[35000:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычислим градиент функции $Q = -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W))$:\n",
    "Сначала посчитаем градиент для одного семпла $x$ и для одного класса $W_i$:\n",
    "\n",
    "$Q(W) = -[y = i]\\cdot\\log(p_i(W)) = -[y = i]\\cdot\\log\\frac{exp(W_{i}x)}{\\sum_{j}exp(W_{j}x)} = [y = i]\\cdot(\\log(\\sum_{j}exp(W_{j}x)) - \\log(exp(W_{i}x))) = [y = i]\\cdot(\\log(\\sum_{j}exp(W_{j}x)) - W_{i}x)$\n",
    "\n",
    "Отсюда $Q'(W_i^k) = [y = i]\\cdot(\\log(\\sum_{j}exp(W_{j}x)) - W_{i}x)' = [y = i]\\cdot(\\frac{\\sum_{j}exp(W_{j}x)'}{\\sum_{j}exp(W_{j}x)} - x_k) = [y = i]\\cdot(\\frac{x_k\\cdot\\sum_{j}exp(W_{j}x)'}{\\sum_{j}exp(W_{j}x)} - x_k) = $\n",
    "\n",
    "$ = x_k\\cdot[y = i]\\cdot(\\frac{\\sum_{j}exp(W_{j}x)'}{\\sum_{j}exp(W_{j}x)} - 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient():\n",
    "    pass\n",
    "\n",
    "def loss_function():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def get_subsets(sample_set, label_set, subset_size=1):\n",
    "    perm = np.random.permutation(len(sample_set))\n",
    "    shuffled_samples = sample_set[perm]\n",
    "    shuffled_labels = label_set[perm]\n",
    "    int_bound = len(shuffled_samples) // subset_size\n",
    "    for i in range(int_bound):\n",
    "        yield ((shuffled_samples[i*subset_size:(i+1)*subset_size],\n",
    "                        shuffled_labels[i*subset_size:(i+1)*subset_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam(samples, labels, gradient_function, delta, delta_coef, delta_coef_degree,\n",
    "         G, G_coef, G_coef_degree, start, step=0.01):\n",
    "    EPSILON = 1e-10\n",
    "    current_location = start.copy()\n",
    "    gradient = gradient_function(samples, labels, start)\n",
    "    delta = delta_coef * delta + (1 - delta_coef) * gradient\n",
    "    G = G_coef * G + (1 - G_coef) * gradient**2\n",
    "    current_location -= (step / (1 - delta_coef_degree) / (1 - G_coef_degree) * delta / (np.sqrt(G) + EPSILON))[0]\n",
    "    delta_coef_degree *= delta_coef\n",
    "    G_coef_degree *= G_coef\n",
    "    return current_location\n",
    "\n",
    "delta_coef = 0.1\n",
    "G_coef = 0.95\n",
    "size = 600\n",
    "current_weights = np.zeros((10, prepared_samples.shape[1]))\n",
    "G = np.zeros_like(current_weights)\n",
    "delta = 0\n",
    "delta_coef_degree = delta_coef\n",
    "G_coef_degree = G_coef\n",
    "steps_number = 0\n",
    "while True:\n",
    "    subsets = get_subsets(prepared_samples, labels, size)\n",
    "    old_weights = current_weights.copy()\n",
    "    for subset in subsets:\n",
    "        current_weights = adam(subset[0], subset[1], gradient, delta, delta_coef, delta_coef_degree,\n",
    "                           G, G_coef, G_coef_degree, current_weights, step=1e-4)\n",
    "    steps_number += 1\n",
    "    if abs(loss_function(current_weights) - loss_function(old_weights) < 1e-3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Полносвязанные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Предложите значения $w$ и $b$, чтобы $y$ реализовала операторы *and*, *or*, *not*.\n",
    "Для начала условимся, что функция Хэвисайда задаётся, как равная единице при $arg \\geq 0$ и равная нулю при $arg < 0$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "*AND*: $\\theta(w_{1}x_1 + w_{2}x_2 + b) = \\theta(x_1 + x_2 - 2)$, т.е. $w_1 = w_2 = 1, b = -2 $\n",
    "\n",
    "*OR*: $\\theta(x_1 + x_2 - 1)$, т.е. $w_1 = w_2 = 1, b = -1 $\n",
    "\n",
    "*NOT($x_1$)*: $\\theta(-x_1)$, т.е. $w_1 = -1, w_2 = b = 0 $\n",
    "2. Приведите пример булевой функции, которая не может быть представлена в виде $y$?\n",
    "\n",
    "     $XOR(0, 0) = 0 \\Rightarrow \\theta(0w_1 + 0w_2 + b) = \\theta(b) = 0 \\Rightarrow b < 0$\n",
    "\n",
    "     $XOR(0, 1) = 1 \\Rightarrow \\theta(w_2 + b) = 1 \\Rightarrow w_2 \\geq -b$\n",
    "     \n",
    "     $XOR(1, 0) = 1 \\Rightarrow \\theta(w_1 + b) = 1 \\Rightarrow w_1 \\geq -b$\n",
    "     \n",
    "     $w_1 \\geq -b, w_2 \\geq -b \\Rightarrow w_1 + w_2 \\geq -2b > -b$\n",
    "     \n",
    "     $XOR(1, 1) = 0 \\Rightarrow \\theta(w_1 + w_2 + b) = 0 \\Rightarrow w_1 + w_2 < -b$, противоречие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть теперь $x \\in \\{0, 1\\}^{n}$, а $y = \\theta(W_2 \\cdot \\theta(W_1x + b_1) + b_2)$\n",
    "\n",
    "Аналогично обычному *AND*, $x \\wedge \\neg y = \\theta(x - y - 2)$, $\\neg x \\wedge y = \\theta(y - x - 2)$\n",
    "\n",
    "Тогда если представить $F = x_1 \\oplus x_2$ как $x_1 \\wedge \\neg x_2 \\vee \\neg x_1 \\wedge x_2$, то $F = \\theta(x_1 - x_2 - 2) \\vee \\theta(x_2 - x_1 - 2) = \\theta(\\theta_1(x_1 - x_2 - 2) + \\theta_2(x_2 - x_1 - 2) - 1)$\n",
    "\n",
    "Содержимое $\\theta_1$ и $\\theta_2$ получается, очевидно умножением соответствующих строк матрицы $W_1$ на вектор $x$ и прибавлением соответствующей компоненты вектора $b_1$, Содержимое внешнего $\\theta$ тогда - сумма $b_2 = -1$ и скалярного произведения вектора $(\\theta_1, \\theta_2)$ на вектор $W_2 = (1, 1)$. Понятно, что это и есть вычисление функции $y$, таким образом, функция выразима с помощью данных средств.\n",
    "\n",
    "Также понятно, что таким образом мы можем выразить ДНФ или КНФ, т.к. функции *AND* и *OR* без каких-либо проблем расширяются для произвольного количества переменных (положительного или отрицательного - $\\neg$ - знака), которыми выражается любая логическая формула"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
