{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multi-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Задание\n",
    "**1. Можно ли вычислить $Q_{t+1}(a)$ инкрементально (известно лишь  $Q_t(a)$ и награда  $r_{t+1}$, назначенная за выбор действия $a$)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Пусть $Q_t(a) = \\frac{\\sum_k r_{k}}{c_t(a)}$. При выборе на $t+1$-ом шаге действия $a:\\; Q_{t+1}(a) =  \\frac{\\sum_k r_{k} \\; + \\; r_{t+1}}{c_t(a) + 1} = \\frac{Q_{t}(a) \\cdot c_t(a) \\; + \\; r_{t+1}}{c_t(a) + 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Построим для начала нашу модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "\n",
    "def log_progress(sequence, every=None, size=None):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{index} / ?'.format(index=index)\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{index} / {size}'.format(\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = str(index or '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MAB_Model:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.reward_means = np.random.normal(size=actions)\n",
    "    def get_reward(self, action):\n",
    "        return np.random.normal(self.reward_means[action])\n",
    "    \n",
    "class MAB_Strategy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.reward_means = np.zeros((model.actions))\n",
    "        self.action_uses = np.zeros((model.actions))\n",
    "    def make_step(self): # Maybe should return reward\n",
    "        pass\n",
    "    def update_reward(self, action, reward):\n",
    "        self.reward_means[action] = (self.reward_means[action] * self.action_uses[action] + reward)\\\n",
    "        / (self.action_uses[action] + 1)\n",
    "        self.action_uses[action] += 1\n",
    "    def best_actions(self):\n",
    "        best =  np.argwhere(self.reward_means == np.max(self.reward_means))\n",
    "        return best.reshape(best.shape[0])\n",
    "    \n",
    "class Player:\n",
    "    def __init__(self, actions, steps, strategy_class, **kwargs):\n",
    "        self.actions = actions\n",
    "        self.strategy_class = strategy_class\n",
    "        self.steps = steps\n",
    "        self.strategy_args = kwargs\n",
    "    def evaluate(self, games=1000, progressbar=True,hold=False, show=True,\n",
    "                 show_opt=True, smoothen=False, color='b', label=' '):\n",
    "        rewards = np.zeros(self.steps)\n",
    "        optimum = np.zeros(self.steps)\n",
    "        if progressbar:\n",
    "            games_range = log_progress(range(games), every=10)\n",
    "        else:\n",
    "            games_range = range(games)\n",
    "        for game in games_range:\n",
    "            model = MAB_Model(self.actions)\n",
    "            strategy = self.strategy_class(model, **(self.strategy_args))\n",
    "            for i in range(self.steps):\n",
    "                rewards[i] += strategy.make_step() / games\n",
    "                optimum[i] += model.reward_means.max() / games\n",
    "        for i in range(1, self.steps):\n",
    "            rewards[i] += rewards[i-1]\n",
    "            optimum[i] += optimum[i-1]\n",
    "        if show:\n",
    "            x = np.arange(1, self.steps+1)\n",
    "            if show_opt:\n",
    "                plt.plot(x, optimum, 'k', label='optimum')\n",
    "            plot_rewards = rewards\n",
    "            if (smoothen):\n",
    "                plot_rewards = rewards.copy()\n",
    "                plot_rewards = gaussian_filter1d(plot_rewards, 10.0)\n",
    "            plt.plot(x, plot_rewards, color, label=label)\n",
    "            plt.title('Total reward', fontsize=16)\n",
    "            if not hold:\n",
    "                plt.show()\n",
    "        return round(rewards.sum() * 100  / optimum.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Реализуем жадную стратегию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Greedy(MAB_Strategy):\n",
    "    def __init__(self, model):\n",
    "        MAB_Strategy.__init__(self, model)\n",
    "    def make_step(self):\n",
    "        best = self.best_actions()\n",
    "        action = np.random.choice(best)\n",
    "        reward = self.model.get_reward(action)\n",
    "        self.update_reward(action, reward)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "player = Player(20, 1000, Greedy)\n",
    "result = player.evaluate(games=1000)\n",
    "print('Total reward:', str(result) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Её основной недостаток достаточно очевиден: сначала действия выбираются абсолютно случайно, и если на первом шаге алгоритм выберет \"плохой\" автомат, но на котором случайно выпал хороший выигрыш, он так и продолжит на нём играть, и переключится на другие автоматы только тогда, когда средний выигрыш на автомате станет неположительным. Кроме того, если выигрыш будет положителен, но мал, алгоритм так и продолжит использовать этот автомат, даже если есть более выгодные автоматы.\n",
    "\n",
    "В качестве начальной стратегии использовался просто случайный выбор автомата.\n",
    "\n",
    "Конечно же, из указанных выше соображений было бы неплохо провести начальную серию игр для того, чтобы собранная информация в большей степени соответствовала реальному матожиданию выигрыша на каждом автомате. Кроме того, очевидно, что у автомата с большим матожиданием выигрыша вероятность выпадения высокого слычайного выигрыша больше, чем у автомата с меньшим матожиданием выигрыша, поэтому выбор автомата с наибольшим выигрышем вместо случайного автомата в среднем должно улучшать результат\n",
    "\n",
    "Для наглядности запустим аналогичную симуляцию, но в которой для каждого запуска алгоритм предварительно дважды дёргал за ручку каждого автомата:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class InitializingPlayer:\n",
    "    def __init__(self, actions, steps, strategy_class, **kwargs):\n",
    "        self.actions = actions\n",
    "        self.strategy_class = strategy_class\n",
    "        self.steps = steps\n",
    "        self.strategy_args = kwargs\n",
    "    def evaluate(self, games=1000, progressbar=True,hold=False, show=True,\n",
    "                 show_opt=True, smoothen=False, color='b', label=' '):\n",
    "        rewards = np.zeros(self.steps)\n",
    "        optimum = np.zeros(self.steps)\n",
    "        if progressbar:\n",
    "            games_range = log_progress(range(games), every=10)\n",
    "        else:\n",
    "            games_range = range(games)\n",
    "        for game in games_range:\n",
    "            model = MAB_Model(self.actions)\n",
    "            strategy = self.strategy_class(model, **(self.strategy_args))\n",
    "            for action in range(model.actions):\n",
    "                reward = strategy.model.get_reward(action)\n",
    "                strategy.update_reward(action, reward)\n",
    "                rewards[action] += reward / games\n",
    "                optimum[action] += model.reward_means.max() / games\n",
    "            for action in range(model.actions):\n",
    "                reward = strategy.model.get_reward(action)\n",
    "                strategy.update_reward(action, reward)\n",
    "                rewards[model.actions + action] += reward / games\n",
    "                optimum[action + model.actions] += model.reward_means.max() / games\n",
    "            for i in range(self.steps - 2*model.actions):\n",
    "                rewards[i + 2*model.actions] += strategy.make_step() / games\n",
    "                optimum[i + 2*model.actions] += model.reward_means.max() / games\n",
    "        for i in range(1, self.steps):\n",
    "            rewards[i] += rewards[i-1]\n",
    "            optimum[i] += optimum[i-1]\n",
    "        if show:\n",
    "            x = np.arange(1, self.steps+1)\n",
    "            if show_opt:\n",
    "                plt.plot(x, optimum, 'k', label='optimum')\n",
    "            plot_rewards = rewards\n",
    "            if (smoothen):\n",
    "                plot_rewards = rewards.copy()\n",
    "                plot_rewards = gaussian_filter1d(plot_rewards, 10.0)\n",
    "            plt.plot(x, plot_rewards, color, label=label)\n",
    "            plt.title('Average reward', fontsize=16)\n",
    "            if not hold:\n",
    "                plt.show()\n",
    "        return round(rewards.sum() * 100  / optimum.sum())\n",
    "\n",
    "player = InitializingPlayer(20, 1000, Greedy)\n",
    "result = player.evaluate(games=1000)\n",
    "print('Total reward:', str(result) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Однако можно заметить, что очень похожего результата можно добиться, если инициализировать вектор средних наград не нулями, а достаточно большими числами (в данном случае - 20), которые с достаточно большой вероятностью будут больше выпадающей награды. Таким образом, с вероятностью, близкой к 1, при первом выборе автомата его средняя награда станет меньше 20. Таким образом, в начале запуска стратегия переберёт все автоматы по одному разу и лишь затем начнёт играть жадно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Greedy10(MAB_Strategy):\n",
    "    def __init__(self, model):\n",
    "        MAB_Strategy.__init__(self, model)\n",
    "        self.reward_means.fill(20)\n",
    "    def make_step(self):\n",
    "        best = self.best_actions()\n",
    "        action = np.random.choice(best)\n",
    "        reward = self.model.get_reward(action)\n",
    "        self.update_reward(action, reward)\n",
    "        return reward\n",
    "player = Player(20, 1000, Greedy10)\n",
    "result = player.evaluate(games=1000)\n",
    "print('Total reward:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### $\\varepsilon$ -greedy стратегия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class E_Greedy(MAB_Strategy):\n",
    "    def __init__(self, model, eps):\n",
    "        MAB_Strategy.__init__(self, model)\n",
    "        self.eps = eps\n",
    "    def make_step(self):\n",
    "        best = (self.reward_means == np.max(self.reward_means))\n",
    "        proba = (1-self.eps) * best / best.sum()\n",
    "        proba += self.eps * np.ones((self.model.actions)) / self.model.actions\n",
    "        action = np.random.choice(self.model.actions, p=proba)\n",
    "        reward = self.model.get_reward(action)\n",
    "        self.update_reward(action, reward)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В данной стратегии $\\varepsilon$ отвечает за \"любознательность\" стратегии: чем значение больше, тем меньше стратегия будет оглядываться на предыдущие результаты и тем активнее выбирать случайный автомат. При $\\varepsilon = 1$ стратегия вообще не будет учитывать средние награды и будет выбирать автоматы случайным образом. При $\\varepsilon = 0$ наоборот, стратегия превратится в обыкновенную жадную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Softmax(MAB_Strategy):\n",
    "    def __init__(self, model, eps):\n",
    "        MAB_Strategy.__init__(self, model)\n",
    "        self.eps = eps\n",
    "    def make_step(self):\n",
    "        proba = np.exp( (self.reward_means - np.max(self.reward_means)) / self.eps)\n",
    "        proba = proba / proba.sum()\n",
    "        action = np.random.choice(self.model.actions, p=proba)\n",
    "        reward = self.model.get_reward(action)\n",
    "        self.update_reward(action, reward)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Здесь тоже при $\\varepsilon \\rightarrow 0$ мы увеличиваем значения степеней в выражении, и\n",
    "$\\pi_{t+1}(a) \\rightarrow \\{a = \\arg \\max Q_t(a)\\}$\n",
    "\n",
    "В то же время при $\\varepsilon \\rightarrow \\infty : \\; \\forall a \\; \\frac {Q_t(a)}{\\varepsilon} \\rightarrow 0,\\; e^{<...>} \\rightarrow 1,\\; \\pi_{t+1}(a) \\rightarrow \\frac{1}{|A|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class UCB(MAB_Strategy):\n",
    "    def __init__(self, model, eps):\n",
    "        MAB_Strategy.__init__(self, model)\n",
    "        self.eps = eps\n",
    "        self.step = 0\n",
    "    def make_step(self):\n",
    "        value = self.reward_means.copy()\n",
    "        value += self.eps * np.sqrt(2 * math.log(self.step + 1) / (self.action_uses + 0.001))\n",
    "        self.step += 1\n",
    "        best =  np.argwhere(value == np.max(value))\n",
    "        best = best.reshape(best.shape[0])\n",
    "        action = np.random.choice(best)\n",
    "        reward = self.model.get_reward(action)\n",
    "        self.update_reward(action, reward)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Тут с $\\varepsilon$ тоже всё очевидно: при $\\varepsilon \\rightarrow \\infty$ важность того, с какой частотой мы использовали автомат, увеличивается и вскоре начинает перевешивать важность известной средней награды, в результате стратегия опять превращается в случайную, причём не в абсолютно случайную, а с повышенной вероятностью выбора малопосещаемых автоматов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ATTENTION: takes a significant amount of time to be processed\n",
    "games_num = 1000\n",
    "\n",
    "player = Player(20, 1000, Greedy)\n",
    "result = player.evaluate(games=games_num, show=False)\n",
    "greedy_sum = result\n",
    "\n",
    "egreedy_epsilons = [0.01, 0.05, 0.075, 0.1, 0.15, 0.2, 0.3, 0.4]\n",
    "egreedy_results = []\n",
    "for e in log_progress(egreedy_epsilons, every=1):\n",
    "    player = Player(20, 1000, E_Greedy, eps=e)\n",
    "    res = player.evaluate(games=games_num, show=False, progressbar=False)\n",
    "    egreedy_results.append(res)\n",
    "    \n",
    "softmax_epsilons = [0.05, 0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
    "softmax_results = []\n",
    "for e in log_progress(softmax_epsilons, every=1):\n",
    "    player = Player(20, 1000, Softmax, eps=e)\n",
    "    res = player.evaluate(games=games_num, show=False, progressbar=False)\n",
    "    softmax_results.append(res)\n",
    "    \n",
    "ucb_epsilons = [0, 0.01, 0.015, 0.02, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5]\n",
    "ucb_results = []\n",
    "for e in log_progress(ucb_epsilons, every=1):\n",
    "    player = Player(20, 1000, UCB, eps=e)\n",
    "    res = player.evaluate(games=games_num, show=False, progressbar=False)\n",
    "    ucb_results.append(res)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title('score', fontsize=16)\n",
    "plt.axis([0, max(egreedy_epsilons[-1], softmax_epsilons[-1], ucb_epsilons[-1]),\n",
    "          0, max(*egreedy_results, *softmax_results, *ucb_results)*1.1])\n",
    "plt.xlabel('epsilon', fontsize=12)\n",
    "plt.ylabel('total reward', fontsize=12)\n",
    "plt.hold(True)\n",
    "plt.plot(ucb_epsilons, ucb_results, 'm', label='UCB', linewidth=1.5)\n",
    "plt.plot(softmax_epsilons, softmax_results, 'b', label='softmax', linewidth=1.5)\n",
    "plt.plot(egreedy_epsilons, egreedy_results, 'g', label='epsilon-greedy', linewidth=1.5)\n",
    "plt.plot([0, max(softmax_epsilons[-1], egreedy_epsilons[-1])], [greedy_sum, greedy_sum],\n",
    "         'r', label='greedy', linewidth=1.5)\n",
    "plt.plot([0, max(softmax_epsilons[-1], egreedy_epsilons[-1])], [100, 100],\n",
    "         'k', label='optimal', linewidth=1.5)\n",
    "plt.hold(False)\n",
    "plt.legend(loc='lower center', fontsize=14, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Таким образом, мы определили наилучшие коэффициенты $\\varepsilon$ для алгоритмов -  $\\varepsilon=0.1$ для $\\varepsilon-greedy$, $\\varepsilon=0.25$ для $softmax$ и $\\varepsilon=0.25$ для $UCB$ и проверили превосходство UCB над прочими алгоритмами. Кроме того, заметно, что $UCB$ гораздо менее чувствителен к изменению параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Градиентный метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Gradient(MAB_Strategy):\n",
    "    def __init__(self, model, lambd):\n",
    "        MAB_Strategy.__init__(self, model)\n",
    "        self.lambd = lambd\n",
    "        self.priorities = np.zeros((model.actions))\n",
    "        self.step = 1\n",
    "    def update_reward(self, action, reward):\n",
    "        old_rewards = self.reward_means.copy()\n",
    "        proba = np.exp(self.priorities - np.max(self.priorities))\n",
    "        proba = proba / proba.sum()\n",
    "        self.reward_means += 1 / self.step * (reward - self.reward_means)\n",
    "        is_current_action_mask = np.zeros((self.model.actions))\n",
    "        is_current_action_mask[action] += 1\n",
    "        self.priorities += self.lambd * (reward - old_rewards) * (is_current_action_mask - proba)\n",
    "    def make_step(self):\n",
    "        proba = np.exp(self.priorities - np.max(self.priorities))\n",
    "        proba = proba / proba.sum()\n",
    "        action = np.random.choice(self.model.actions, p=proba)\n",
    "        reward = self.model.get_reward(action)\n",
    "        self.update_reward(action, reward)\n",
    "        self.step += 1\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grad_epsilons = [0, 0.01, 0.015, 0.02, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5]\n",
    "grad_results = []\n",
    "for e in log_progress(grad_epsilons, every=1):\n",
    "    player = Player(20, 1000, Gradient, lambd=e)\n",
    "    res = player.evaluate(games=1000, show=False, progressbar=False)\n",
    "    grad_results.append(res)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title('score', fontsize=16)\n",
    "plt.xlabel('lambda', fontsize=12)\n",
    "plt.ylabel('total reward', fontsize=12)\n",
    "plt.hold(True)\n",
    "plt.plot(grad_epsilons, grad_results, 'b', label='Grad', linewidth=1.5)\n",
    "plt.plot([0, grad_epsilons[-1]], [100, 100],\n",
    "         'k', label='optimal', linewidth=1.5)\n",
    "plt.hold(False)\n",
    "plt.legend(loc='lower center', fontsize=14, ncol=2)\n",
    "plt.show()\n",
    "print(\"Max result:\", str(max(grad_results)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Отсюда выберем параметр $\\lambda = 0.2$. Сравним теперь этот метод с предыдущими в динамике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "games_num = 1000\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel(\"total reward at n'th step\")\n",
    "plt.hold(True)\n",
    "greedy_player = Player(20, 1000, Greedy)\n",
    "greedy_player.evaluate(games=games_num, show=True, hold=True, color='r', label='greedy', show_opt=True)\n",
    "e_greedy_player = Player(20, 1000, E_Greedy, eps=0.1)\n",
    "e_greedy_player.evaluate(games=games_num, show=True, hold=True, color='g', label='e-greedy', show_opt=False)\n",
    "softmax_player = Player(20, 1000, Softmax, eps=0.25)\n",
    "softmax_player.evaluate(games=games_num, show=True, hold=True, color='b', label='softmax', show_opt=False)\n",
    "ucb_player = Player(20, 1000, UCB, eps=0.25)\n",
    "ucb_player.evaluate(games=games_num, show=True, hold=True, color='m', label='UCB', show_opt=False)\n",
    "grad_player = Player(20, 1000, Gradient, lambd=0.2)\n",
    "grad_player.evaluate(games=games_num, show=True, hold=True, color='y', label='gradient', show_opt=False)\n",
    "plt.hold(False)\n",
    "plt.legend(loc='upper left', ncol=2, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
