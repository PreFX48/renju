{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Марковский процесс принятия решения (markov decison process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Задание\n",
    "#### 1. Приведите жизненный пример марковского процесса принятия решения (это может быть какаю-нибудь игра и т.п.).\n",
    "Очень простой пример - шахматы. Конечное, пусть и очень большое, количество состояний $S$, конечное множество действий $ A \\subset S \\times S$. Среда - наш противник - отвечает вполне себе случайно. Если рассматривать среду именно как противника, то она, конечно, отдаёт сильное предпочтение тем действиям, которые она считает выигрышными. Награда для каждого допустимого действия из $\\; S \\times A \\;$ нулевая за исключением последнего хода, по результатам которого присуждается награда $\\pm 1$. Кроме того, очевидно, что для принятия решения информация о предыдущих состояниях нерелевантна. Таким образом, принятие решения в шахматах можно смоделировать с помощью марковского процесса.\n",
    "#### 2. Можете ли вы привести пример игры, где принятие решения нельзя смоделировать с помощью марковского процесса?\n",
    "Крестики-нолики на бесконечной доске не могут быть смоделированы хотя бы из-за бесконечности (в частности, континуальности) множества состояний $S$.\n",
    "#### 3. Выведите следующие значения через $p(s_{t+1}, r_{t+1}|s_t, a_t)$, для простоты все распределения можно считать дискретными\n",
    "  * $r(s_{t}, a_{t}) = \\mathbb{E}[R_{t+1}|S_t = s_t, A_t = a_t]$ - средняя награда за действие $a_t$ в $s_t$ \n",
    "  * $p(s_{t+1} | s_t, a_t) = \\Pr\\{S_{t+1} = s_{t+1} | S_t = s_t, A_t = a_t \\}$ - вероятность попасть в $s_{t+1}$ из $s_t$, сделав $a_t$.\n",
    "  * $r(s_t, a_t, s_{t+1}) = \\mathbb{E}[R_{t+1}|S_{t+1} = s_{t+1}, S_t = s_t, A_t = a_t]$ - средняя награда при переезде из $s_t$ в $s_{t+1}$, сделав $a_t$.\n",
    "\n",
    "$r(s_{t}, a_{t}) = \\mathbb{E}[R_{t+1}|S_t = s_t, A_t = a_t] = \\sum_{s,r} r \\cdot p(s, r \\;|\\; s_t, a_t)$\n",
    "\n",
    "$p(s_{t+1} | s_t, a_t) = \\Pr\\{S_{t+1} = s_{t+1} | S_t = s_t, A_t = a_t \\} = \\sum_{r} p(s_{t+1}, r \\;|\\; s_t, a_t)$\n",
    "\n",
    "$r(s_t, a_t, s_{t+1}) = \\mathbb{E}[R_{t+1}|S_{t+1} = s_{t+1}, S_t = s_t, A_t = a_t] = \\sum_{r} r \\cdot p(s_{t+1}, r \\;|\\; s_t, a_t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Смоделируем среду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "\n",
    "def log_progress(sequence, every=None, size=None):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{index} / ?'.format(index=index)\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{index} / {size}'.format(\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = str(index or '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, states, actions):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.reward_means = np.random.normal(0, 1, size=(states, actions))\n",
    "        transitions = np.random.randint(0, 2, size=(states, actions, states), dtype='int8')\n",
    "        transitions[:, :, 0] += 1 - transitions.max(2)\n",
    "        self.transition_probas = np.random.random_sample((states, actions, states)).astype('float64') * transitions\n",
    "        for state in self.transition_probas:\n",
    "            for action in state:\n",
    "                if np.absolute(action).sum() < 0.01:\n",
    "                    action = np.ones(states)\n",
    "                action /= action.sum()\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        return (np.random.choice(self.states, p=self.transition_probas[state, action]),\n",
    "            np.random.normal(self.reward_means[state, action], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PolicyIterationStrategy:\n",
    "    def __init__(self, env, discount):\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.policy = np.ones((env.states, env.actions))\n",
    "        self.policy /= env.actions\n",
    "    def learn(self, steps):\n",
    "        state_values = np.random.randn(self.env.states)\n",
    "        state_action_values = np.zeros((self.env.states, self.env.actions))\n",
    "        i = 0\n",
    "        while i < steps: # TODO: decide when to stop\n",
    "            for state in range(self.env.states):\n",
    "                val_sum = 0\n",
    "                for action in range(self.env.actions):\n",
    "                    action_sum = 0\n",
    "                    for new_state in range(self.env.states):\n",
    "                        action_sum += self.env.transition_probas[state, action, new_state] * \\\n",
    "                            (self.env.reward_means[state, action] + \\\n",
    "                                self.discount*state_values[new_state])\n",
    "                    val_sum += self.policy[state, action] * action_sum\n",
    "                    state_action_values[state, action] = action_sum\n",
    "                state_values[state] = val_sum\n",
    "                comp_values = state_action_values[state]\n",
    "                best = np.argwhere(comp_values == comp_values.max())\n",
    "                self.policy[state] = 0\n",
    "                self.policy[state][best] = 1\n",
    "                self.policy[state] /= self.policy[state].sum()\n",
    "                i += 1\n",
    "                if i >= steps:\n",
    "                    break\n",
    "                \n",
    "    def choose(self, state):\n",
    "        return np.random.choice(self.env.actions, p=self.policy[state])\n",
    "    \n",
    "\n",
    "class ValueIterationStrategy:\n",
    "    def __init__(self, env, discount):\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.policy = np.ones((env.states, env.actions))\n",
    "        self.policy /= env.actions\n",
    "    def learn(self, steps):\n",
    "        state_values = np.random.randn(self.env.states)\n",
    "        state_action_values = np.zeros((self.env.states, self.env.actions))\n",
    "        i = 0\n",
    "        while i < steps: # TODO: decide when to stop\n",
    "            for state in range(self.env.states):\n",
    "                max_val = -1e20\n",
    "                for action in range(self.env.actions):\n",
    "                    action_sum = 0\n",
    "                    for new_state in range(self.env.states):\n",
    "                        action_sum += self.env.transition_probas[state, action, new_state] * \\\n",
    "                            (self.env.reward_means[state, action] + \\\n",
    "                                self.discount*state_values[new_state])\n",
    "                    max_val = max(max_val, action_sum)\n",
    "                    state_action_values[state, action] = action_sum\n",
    "                state_values[state] = max_val\n",
    "                comp_values = state_action_values[state]\n",
    "                best = np.argwhere(comp_values == comp_values.max())\n",
    "                self.policy[state] = 0\n",
    "                self.policy[state][best] = 1\n",
    "                self.policy[state] /= self.policy[state].sum()\n",
    "                i += 1\n",
    "                if i >= steps:\n",
    "                    break\n",
    "                \n",
    "    def choose(self, state):\n",
    "        return np.random.choice(self.env.actions, p=self.policy[state])\n",
    "    \n",
    "class RandomStrategy:\n",
    "    def __init__(self, env, discount):\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.policy = np.ones((env.states, env.actions))\n",
    "        self.policy /= env.actions\n",
    "    def learn(self, steps):\n",
    "        pass\n",
    "    def choose(self, state):\n",
    "        return np.random.choice(self.env.actions, p=self.policy[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MarkovPlayer:\n",
    "    def __init__(self, states, actions, steps, strategy_class, **kwargs):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.strategy_class = strategy_class\n",
    "        self.steps = steps\n",
    "        self.strategy_args = kwargs\n",
    "    def evaluate(self, games=1000, learning_time=100, progressbar=True,hold=False, show=True,\n",
    "                 color='b', label=' '):\n",
    "        rewards = np.zeros(self.steps)\n",
    "#         env = global_env\n",
    "        if progressbar:\n",
    "            games_range = log_progress(range(games), every=1)\n",
    "        else:\n",
    "            games_range = range(games)\n",
    "        for game in games_range:\n",
    "            state = 0\n",
    "            self.state = state\n",
    "            env = Environment(self.states, self.actions)\n",
    "            strategy = self.strategy_class(env, **(self.strategy_args))\n",
    "            self.strategy = strategy\n",
    "            strategy.learn(learning_time)\n",
    "            for i in range(self.steps):\n",
    "                state, rewards[i] = env.step(state, strategy.choose(state))\n",
    "                self.state = state\n",
    "        for i in range(1, self.steps):\n",
    "            rewards[i] += rewards[i-1]\n",
    "        if show:\n",
    "            x = np.arange(1, self.steps+1)\n",
    "            plot_rewards = rewards\n",
    "            plt.plot(x, plot_rewards, color, label=label)\n",
    "            plt.title('Total reward', fontsize=16)\n",
    "            if not hold:\n",
    "                plt.show()\n",
    "        return rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:11: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py:917: UserWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  warnings.warn(self.msg_depr_set % key)\n",
      "/usr/local/lib/python3.5/dist-packages/matplotlib/rcsetup.py:152: UserWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  warnings.warn(\"axes.hold is deprecated, will be removed in 3.0\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8c159f6eef73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpolicy_iteration_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgames_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpolicy_iteration_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgames_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpolicy_iteration_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgames_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ea5ae0dd5727>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, games, learning_time, progressbar, hold, show, color, label)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-627706cd110c>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0maction_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                         \u001b[0maction_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_probas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m                             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0mval_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mstate_action_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "games_num = 200\n",
    "random_player = MarkovPlayer(states=100, actions=20, steps=1000,\n",
    "                           strategy_class=RandomStrategy, discount=1.0)\n",
    "policy_iteration_player = MarkovPlayer(states=100, actions=20, steps=1000,\n",
    "                           strategy_class=PolicyIterationStrategy, discount=1.0)\n",
    "value_iteration_player = MarkovPlayer(states=100, actions=20, steps=1000,\n",
    "                           strategy_class=ValueIterationStrategy, discount=1.0)\n",
    "# global_env = Environment(100, 20)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(121)\n",
    "plt.hold(True)\n",
    "random_player.evaluate(games=games_num, hold=True, color='k')\n",
    "policy_iteration_player.evaluate(games=games_num, learning_time=1, hold=True, color='r')\n",
    "policy_iteration_player.evaluate(games=games_num, learning_time=10, hold=True, color='g')\n",
    "policy_iteration_player.evaluate(games=games_num, learning_time=25, hold=True, color='b')\n",
    "policy_iteration_player.evaluate(games=games_num, learning_time=100, hold=True, color='m')\n",
    "plt.hold(False)\n",
    "plt.axis([0, 1000, 0, 2000])\n",
    "plt.subplot(122)\n",
    "plt.hold(True)\n",
    "random_player.evaluate(games=games_num, hold=True, color='k')\n",
    "value_iteration_player.evaluate(games=games_num, learning_time=1, hold=True, color='r')\n",
    "value_iteration_player.evaluate(games=games_num, learning_time=10, hold=True, color='g')\n",
    "value_iteration_player.evaluate(games=games_num, learning_time=25, hold=True, color='b')\n",
    "value_iteration_player.evaluate(games=games_num, learning_time=100, hold=True, color='m')\n",
    "plt.hold(False)\n",
    "plt.axis([0, 1000, 0, 2000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print(policy_iteration_player.env.state, policy_iteration_player.env.action)\n",
    "# print(policy_iteration_player.env.transition_probas[0, 0])\n",
    "print(policy_iteration_player.strategy.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ATTENTION: takes a significant amount of time to be processed\n",
    "games_num = 5\n",
    "    \n",
    "# learning_steps = [1, 2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "learning_steps = [1, 2, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31]\n",
    "policy_results = []\n",
    "for e in log_progress(learning_steps, every=1):\n",
    "    player = MarkovPlayer(states=100, actions=20, steps=1000,\n",
    "                           strategy_class=PolicyIterationStrategy, discount=1.0)\n",
    "    res = player.evaluate(games=games_num, learning_time=e, show=False, progressbar=False)\n",
    "    policy_results.append(res)\n",
    "    \n",
    "value_results = []\n",
    "for e in log_progress(learning_steps, every=1):\n",
    "    player = MarkovPlayer(states=100, actions=20, steps=1000,\n",
    "                           strategy_class=ValueIterationStrategy, discount=1.0)\n",
    "    res = player.evaluate(games=games_num, learning_time=e, show=False, progressbar=False)\n",
    "    value_results.append(res)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title('score', fontsize=16)\n",
    "plt.axis([0, learning_steps[-1],\n",
    "          0, max(*policy_results, *value_results)*1.1])\n",
    "plt.xlabel('learning games', fontsize=12)\n",
    "plt.ylabel('total reward', fontsize=12)\n",
    "plt.hold(True)\n",
    "plt.plot(learning_steps, policy_results, 'r', label='Policy', linewidth=1.5)\n",
    "plt.plot(learning_steps, value_results, 'b', label='Value', linewidth=1.5)\n",
    "plt.hold(False)\n",
    "plt.legend(loc='lower center', fontsize=14, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  },
  "widgets": {
   "state": {
    "07100a2d44e64d0bb1fe3c03bc8e9fce": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "177c1bed61264af0bb53d58a8978c999": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "1fef3fd78fba4969a09a2ba05fb1b6ee": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "295a78e3340d49c48c27b81204e264b4": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "341e1a52277d4066b41155d7df108377": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "34514d7a03a84c688702dafb0ef1792c": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "34875d5c0b054a89a18c91045961e28c": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "35544d89ef9f4c5a8534ad45d17a8582": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "389320415ca648488ae2b4efbb6b17ff": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "3ffee6377e1b4d79bcb63aa37641500f": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "4257a74bca8d421a94c9cf8098cecb75": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "43217692f0004fe69ff2160160f1391a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "5130576923aa44b1961362dc24592cd2": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "5bf113a7b0ce4c6b9a3f01b128327957": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "5d35ca1176dd4595b0325f8e99c8bd84": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "656f39febf624c7cb2489a0794ca729a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "6c6cd99345694a0ab31bf3f03eb743aa": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "799e89c801f2435989a37d930c523c61": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "79e0b4efa1d0423096523c995a2dcfd9": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "85ecfa29d5b5477f9afbbb125e516805": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "885d1aef954b4da183032ea43427a586": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "9bb1937486aa4ff9b0c5820efec1e6d8": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "9dd7dd74a8e847de98b815c850ac5826": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "a08a9e5d10d043fa9d7e6a89bd73d26a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "a145f2a35ae14f18b0f856ad438722b5": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "a1b3df928f724fc7a36683e62c520a3b": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "a59ff2124fb5493f8a020a8cc01abe1b": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "acd8652fbe7740439e46a39566a803f1": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "ae22ec5a8d1a402d951aef00c5cc5787": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "b675f3e6f1504c55a0b4d71dfb96ed03": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "c09739735f8b4a329fa111e8ed152f0c": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "c1670ca6fe654cd9885bc3d0c18573ba": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "c5844c551cd14a6a9be5df14ee7228f5": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "c94f5a57bd554192abe81d7e3f5a1d5f": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "ca83b36b311a4afe838e17dc727bad90": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "cd6f42b47cda408bb1a3eb7462b28081": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "e0d67c23e19c47aa8ddd3761d0bb8209": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "ec6711e2af434cb8ab34e619349864c4": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "ece755afad8d4099a1d9d815aec33ef8": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "f85b137b28504784a953b69ba423ef2f": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "fd14c14b4014413bbed1f9ab2bdfec89": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "fdcad1e79feb41ec8ef1f80256b967f8": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
